{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbQ7QCCcMZ0x"
      },
      "outputs": [],
      "source": [
        "#installing the required libraries\n",
        "\n",
        "!pip install transformers datasets seqeval matplotlib torch pandas numpy -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nn21Yn1tJETT",
        "outputId": "b7d33672-3f6c-42ad-9133-f6b9ca58d5c3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "from transformers import EvalPrediction\n",
        "from transformers import TrainingArguments\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "from transformers import Trainer\n",
        "from transformers import AutoConfig\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "from collections import defaultdict\n",
        "from collections import Counter\n",
        "\n",
        "import torch.nn as nn\n",
        "from matplotlib import pyplot as plt\n",
        "from transformers import XLMRobertaConfig\n",
        "from transformers.modeling_outputs import TokenClassifierOutput\n",
        "from transformers.models.roberta.modeling_roberta import RobertaModel\n",
        "from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uaI6rEzCJNUI"
      },
      "outputs": [],
      "source": [
        "\n",
        "from seqeval.metrics import f1_score\n",
        "from seqeval.metrics import classification_report\n",
        "\n",
        "\n",
        "from datasets import get_dataset_config_names\n",
        "from datasets import load_dataset\n",
        "from datasets import DatasetDict\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing the required Data and Preprocessing"
      ],
      "metadata": {
        "id": "anpic4SRM_uc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcsCVTEsLwFd"
      },
      "source": [
        "XTREME is a benchmark for the evaluation of the cross-lingual generalization ability of pre-trained multilingual models that covers 40 typologically diverse languages and includes nine tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_C3x-jeNLw-9",
        "outputId": "ec8c2dea-51aa-4521-d956-70fbaa5e47d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XTREME has 183 configurations.\n"
          ]
        }
      ],
      "source": [
        "xtreme_subsets = get_dataset_config_names(\"xtreme\")\n",
        "print(f\"XTREME has {len(xtreme_subsets)} configurations.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNatJ6f3MAZp",
        "outputId": "e3626924-7b38-4d93-dda5-7a97ddd16f9c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['PAN-X.af',\n",
              " 'PAN-X.ar',\n",
              " 'PAN-X.bg',\n",
              " 'PAN-X.bn',\n",
              " 'PAN-X.de',\n",
              " 'PAN-X.el',\n",
              " 'PAN-X.en',\n",
              " 'PAN-X.es',\n",
              " 'PAN-X.et',\n",
              " 'PAN-X.eu',\n",
              " 'PAN-X.fa',\n",
              " 'PAN-X.fi',\n",
              " 'PAN-X.fr',\n",
              " 'PAN-X.he',\n",
              " 'PAN-X.hi',\n",
              " 'PAN-X.hu',\n",
              " 'PAN-X.id',\n",
              " 'PAN-X.it',\n",
              " 'PAN-X.ja',\n",
              " 'PAN-X.jv',\n",
              " 'PAN-X.ka',\n",
              " 'PAN-X.kk',\n",
              " 'PAN-X.ko',\n",
              " 'PAN-X.ml',\n",
              " 'PAN-X.mr',\n",
              " 'PAN-X.ms',\n",
              " 'PAN-X.my',\n",
              " 'PAN-X.nl',\n",
              " 'PAN-X.pt',\n",
              " 'PAN-X.ru',\n",
              " 'PAN-X.sw',\n",
              " 'PAN-X.ta',\n",
              " 'PAN-X.te',\n",
              " 'PAN-X.th',\n",
              " 'PAN-X.tl',\n",
              " 'PAN-X.tr',\n",
              " 'PAN-X.ur',\n",
              " 'PAN-X.vi',\n",
              " 'PAN-X.yo',\n",
              " 'PAN-X.zh']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#fetching the pan-x configurations from all the xtreme configurations\n",
        "panx_subsets = {s for s in xtreme_subsets if s.startswith(\"PAN\")}\n",
        "panx_subsets = sorted(panx_subsets)\n",
        "panx_subsets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apvhk8nHMVoQ",
        "outputId": "9b9d454b-4f82-4b27-fe8d-c6497a5a90ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['tokens', 'ner_tags', 'langs'],\n",
              "        num_rows: 20000\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['tokens', 'ner_tags', 'langs'],\n",
              "        num_rows: 10000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['tokens', 'ner_tags', 'langs'],\n",
              "        num_rows: 10000\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#according to wiki, english is most spoken language (non native speakers included) in europe w 260m speakers\n",
        "load_dataset('xtreme', name='PAN-X.en')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will fine tune our model on english, and then perform zero shot learning with the other 4 most widely spoken european language."
      ],
      "metadata": {
        "id": "TXcy5TE_NK1f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rtKKGVQMsC2",
        "outputId": "392b89f6-c7a5-4f12-d73f-9fc5729413ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[32.581453634085214, 26.31578947368421, 21.303258145363408, 10.275689223057643, 9.523809523809524]\n"
          ]
        }
      ],
      "source": [
        "languages = ['en','fr','de','it','es']\n",
        "speakers = [260,210,170,82,76] #in millions\n",
        "def convert_to_percentage(arr):\n",
        "  total = sum(arr)\n",
        "  return [100 * count / total for count in arr]\n",
        "percentages = convert_to_percentage(speakers)\n",
        "print(percentages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLFVSD7LQ5aD"
      },
      "outputs": [],
      "source": [
        "#loading datasets for the selected languages, shuffling and selecting a percentage of samples, then storing them in a combined dataset dictionary.\n",
        "panx_ds_combined = defaultdict(DatasetDict)\n",
        "\n",
        "for lang, per in zip(languages,percentages):\n",
        "  ds = load_dataset('xtreme', name=f'PAN-X.{lang}')\n",
        "  for train_test_val in ds:\n",
        "    num_samples = min(int(per/100 * len(ds[train_test_val])), len(ds[train_test_val]))\n",
        "    panx_ds_combined[lang][train_test_val] = (\n",
        "        ds[train_test_val].shuffle(seed=42).select(range(num_samples)) #shuffle to ensure we dont accidentally bias our splits. it randomly shuffles the col values. #select returns rows acc to list of indices\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehOdN6cwSbWp",
        "outputId": "0cee623a-a7e7-417e-b405-443becd38115"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "defaultdict(datasets.dataset_dict.DatasetDict,\n",
              "            {'en': DatasetDict({\n",
              "                 train: Dataset({\n",
              "                     features: ['tokens', 'ner_tags', 'langs'],\n",
              "                     num_rows: 6516\n",
              "                 })\n",
              "                 validation: Dataset({\n",
              "                     features: ['tokens', 'ner_tags', 'langs'],\n",
              "                     num_rows: 3258\n",
              "                 })\n",
              "                 test: Dataset({\n",
              "                     features: ['tokens', 'ner_tags', 'langs'],\n",
              "                     num_rows: 3258\n",
              "                 })\n",
              "             }),\n",
              "             'fr': DatasetDict({\n",
              "                 train: Dataset({\n",
              "                     features: ['tokens', 'ner_tags', 'langs'],\n",
              "                     num_rows: 5263\n",
              "                 })\n",
              "                 validation: Dataset({\n",
              "                     features: ['tokens', 'ner_tags', 'langs'],\n",
              "                     num_rows: 2631\n",
              "                 })\n",
              "                 test: Dataset({\n",
              "                     features: ['tokens', 'ner_tags', 'langs'],\n",
              "                     num_rows: 2631\n",
              "                 })\n",
              "             }),\n",
              "             'de': DatasetDict({\n",
              "                 train: Dataset({\n",
              "                     features: ['tokens', 'ner_tags', 'langs'],\n",
              "                     num_rows: 4260\n",
              "                 })\n",
              "                 validation: Dataset({\n",
              "                     features: ['tokens', 'ner_tags', 'langs'],\n",
              "                     num_rows: 2130\n",
              "                 })\n",
              "                 test: Dataset({\n",
              "                     features: ['tokens', 'ner_tags', 'langs'],\n",
              "                     num_rows: 2130\n",
              "                 })\n",
              "             }),\n",
              "             'it': DatasetDict({\n",
              "                 train: Dataset({\n",
              "                     features: ['tokens', 'ner_tags', 'langs'],\n",
              "                     num_rows: 2055\n",
              "                 })\n",
              "                 validation: Dataset({\n",
              "                     features: ['tokens', 'ner_tags', 'langs'],\n",
              "                     num_rows: 1027\n",
              "                 })\n",
              "                 test: Dataset({\n",
              "                     features: ['tokens', 'ner_tags', 'langs'],\n",
              "                     num_rows: 1027\n",
              "                 })\n",
              "             }),\n",
              "             'es': DatasetDict({\n",
              "                 train: Dataset({\n",
              "                     features: ['tokens', 'ner_tags', 'langs'],\n",
              "                     num_rows: 1904\n",
              "                 })\n",
              "                 validation: Dataset({\n",
              "                     features: ['tokens', 'ner_tags', 'langs'],\n",
              "                     num_rows: 952\n",
              "                 })\n",
              "                 test: Dataset({\n",
              "                     features: ['tokens', 'ner_tags', 'langs'],\n",
              "                     num_rows: 952\n",
              "                 })\n",
              "             })})"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "panx_ds_combined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "82clM3YUSj76",
        "outputId": "0dc04e9c-36ad-4443-9a95-90a0586c9b1d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>en</th>\n",
              "      <th>fr</th>\n",
              "      <th>de</th>\n",
              "      <th>it</th>\n",
              "      <th>es</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6516</td>\n",
              "      <td>5263</td>\n",
              "      <td>4260</td>\n",
              "      <td>2055</td>\n",
              "      <td>1904</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     en    fr    de    it    es\n",
              "0  6516  5263  4260  2055  1904"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.DataFrame({lang: [panx_ds_combined[lang]['train'].num_rows] for lang in languages})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJ4bUdHiTjp5"
      },
      "outputs": [],
      "source": [
        "#fine tuning xlm roberta on english. later we'll do the cross lingual zero shot learning on the other datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rObkHcCTqyQ",
        "outputId": "7a105505-aaf5-49b0-caf5-8bb3ca8bfc75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokens:[\"''\", 'January', '21', \"''\", '–', 'Nanny', 'and', 'the', 'Professor']\n",
            "ner_tags:[0, 0, 0, 0, 0, 1, 2, 2, 2]\n",
            "langs:['en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en']\n"
          ]
        }
      ],
      "source": [
        "example = panx_ds_combined['en']['train'][0]\n",
        "for key,val in example.items():\n",
        "  print(f\"{key}:{val}\") #key column name of arrow table. val is entry in each column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4ydNO4CUQn-",
        "outputId": "0ea5bc1f-1a55-4a8f-ed6b-8814c1ac1331"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokens:Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)\n",
            "ner_tags:Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], id=None), length=-1, id=None)\n",
            "langs:Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)\n"
          ]
        }
      ],
      "source": [
        "#checking underlying datatypes.\n",
        "for key,val in panx_ds_combined['en'][\"train\"].features.items():\n",
        "  print(f\"{key}:{val}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHtn9Ur8Uxwv",
        "outputId": "f398f678-e549-4f1d-d07f-951d0b3491de"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], id=None)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#seq class specifies the field contains a list of features. so, for ner tags it is a list of class labels\n",
        "tags = panx_ds_combined['en']['train'].features[\"ner_tags\"].feature\n",
        "tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4T1aOwTpVHoA",
        "outputId": "c8135684-5049-446e-bdf6-9fa88904048d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['tokens', 'ner_tags', 'langs', 'ner_tags_str'],\n",
              "        num_rows: 6516\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['tokens', 'ner_tags', 'langs', 'ner_tags_str'],\n",
              "        num_rows: 3258\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['tokens', 'ner_tags', 'langs', 'ner_tags_str'],\n",
              "        num_rows: 3258\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#converting numerical NER tag indices to their string representations for the English dataset and mapping the function to the dataset\n",
        "def create_tag_names(batch):\n",
        "  return {\"ner_tags_str\":[tags.int2str(idx) for idx in batch['ner_tags']]}\n",
        "\n",
        "panx_en = panx_ds_combined['en'].map(create_tag_names)\n",
        "panx_en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZyLbrHPVon0",
        "outputId": "96ac9f7f-a991-4281-a1f0-69821d877b3e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'tokens': [\"''\",\n",
              "  'January',\n",
              "  '21',\n",
              "  \"''\",\n",
              "  '–',\n",
              "  'Nanny',\n",
              "  'and',\n",
              "  'the',\n",
              "  'Professor'],\n",
              " 'ner_tags': [0, 0, 0, 0, 0, 1, 2, 2, 2],\n",
              " 'langs': ['en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en'],\n",
              " 'ner_tags_str': ['O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'I-PER']}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "panx_en['train'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "ykef6q_EVw3n",
        "outputId": "5fbca906-53ce-4b5e-be2d-95e4aae832fd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Tokens</th>\n",
              "      <td>''</td>\n",
              "      <td>January</td>\n",
              "      <td>21</td>\n",
              "      <td>''</td>\n",
              "      <td>–</td>\n",
              "      <td>Nanny</td>\n",
              "      <td>and</td>\n",
              "      <td>the</td>\n",
              "      <td>Professor</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Tags</th>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>B-PER</td>\n",
              "      <td>I-PER</td>\n",
              "      <td>I-PER</td>\n",
              "      <td>I-PER</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         0        1   2   3  4      5      6      7          8\n",
              "Tokens  ''  January  21  ''  –  Nanny    and    the  Professor\n",
              "Tags     O        O   O   O  O  B-PER  I-PER  I-PER      I-PER"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "en_example = panx_en['train'][0]\n",
        "pd.DataFrame([en_example['tokens'], en_example['ner_tags_str']], ['Tokens','Tags'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "6DvGnuhqWMLz",
        "outputId": "a8ca6263-001e-4ddf-81bb-1a2cd0bf086e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PER</th>\n",
              "      <th>LOC</th>\n",
              "      <th>ORG</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>train</th>\n",
              "      <td>2983</td>\n",
              "      <td>3180</td>\n",
              "      <td>3089</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>validation</th>\n",
              "      <td>1518</td>\n",
              "      <td>1511</td>\n",
              "      <td>1576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>test</th>\n",
              "      <td>1542</td>\n",
              "      <td>1530</td>\n",
              "      <td>1574</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             PER   LOC   ORG\n",
              "train       2983  3180  3089\n",
              "validation  1518  1511  1576\n",
              "test        1542  1530  1574"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#counting ner tag splits for each 3\n",
        "\n",
        "def get_ner_tag_counts(dataset):\n",
        "  split2freqs = defaultdict(Counter)\n",
        "  for split, ds in dataset.items():\n",
        "    for row in ds['ner_tags_str']:\n",
        "      for tag in row:\n",
        "        if tag.startswith('B'):\n",
        "          tag_type = tag.split('-')[1]\n",
        "          split2freqs[split][tag_type] += 1\n",
        "  return split2freqs\n",
        "\n",
        "frequencies = get_ner_tag_counts(panx_en)\n",
        "\n",
        "pd.DataFrame.from_dict(frequencies, orient=\"index\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THeRJdlsXyog"
      },
      "source": [
        "# Creating a custom model for token classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osupSlR7X56R"
      },
      "outputs": [],
      "source": [
        "class XLMRobertaforTokenClassification(RobertaPreTrainedModel):\n",
        "  config_class = XLMRobertaConfig\n",
        "\n",
        "  def __init__(self, config):\n",
        "    super().__init__(config) #initializing the parent class with the given configuration\n",
        "    self.num_labels = config.num_labels\n",
        "\n",
        "    self.roberta = RobertaModel(config, add_pooling_layer=False) #initialising the roberta model\n",
        "    self.dropout = nn.Dropout(config.hidden_dropout_prob) #drop layer to prevent overfitting\n",
        "    self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "    self.init_weights()\n",
        "\n",
        "  def forward(self,input_ids=None,attention_mask=None,token_type_ids=None,labels=None, **kwargs):\n",
        "    outputs = self.roberta(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, **kwargs) #forward pass through the roberta model\n",
        "    sequence_output = self.dropout(outputs[0]);\n",
        "    logits = self.classifier(sequence_output) #computing logits for each token in the sequence\n",
        "\n",
        "    #calculating loss if labels are provided\n",
        "    loss = None\n",
        "    if labels is not None:\n",
        "      loss_fun = nn.CrossEntropyLoss()\n",
        "      loss = loss_fun(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "    #returning the loss, logits, hidden states, and attention weights\n",
        "    return TokenClassifierOutput(loss = loss, logits = logits, hidden_states = outputs.hidden_states, attentions = outputs.attentions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhrTCNnOpGtn",
        "outputId": "debfa688-0bca-4c79-ac75-d643eb8d2152"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: 'O',\n",
              " 1: 'B-PER',\n",
              " 2: 'I-PER',\n",
              " 3: 'B-ORG',\n",
              " 4: 'I-ORG',\n",
              " 5: 'B-LOC',\n",
              " 6: 'I-LOC'}"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#creating dictionaries mapping index to tag and vice versa\n",
        "index2tag = {idx: tag for idx, tag in enumerate(tags.names)}\n",
        "tag2index = {tag: idx for idx, tag in enumerate(tags.names)}\n",
        "index2tag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7VChcq8pX2D",
        "outputId": "70a119ee-6628-4c22-9d4a-bc425c14dc11"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'O': 0,\n",
              " 'B-PER': 1,\n",
              " 'I-PER': 2,\n",
              " 'B-ORG': 3,\n",
              " 'I-ORG': 4,\n",
              " 'B-LOC': 5,\n",
              " 'I-LOC': 6}"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tag2index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgRROFDwpa-5"
      },
      "outputs": [],
      "source": [
        "#we'll store this in autoconfig class (with modified parameters)\n",
        "\n",
        "xlmr_model_name = 'xlm-roberta-base'\n",
        "xlmr_tokenizer = AutoTokenizer.from_pretrained(xlmr_model_name) #loading the tokenizer for the specified XLM-Roberta model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8aaSqp0p1ZZ",
        "outputId": "ffec1260-2759-4679-8eff-4d47ba30f574"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "#loading the configuration for the specified XLM-Roberta model\n",
        "#with the number of labels, and mappings from id to label and label to id\n",
        "xlmr_config = AutoConfig.from_pretrained(xlmr_model_name, num_labels = tags.num_classes, id2label = index2tag, label2id = tag2index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yi6gIt6TqcAx",
        "outputId": "68a408ab-c60c-4dfd-9e29-089d6b4d1466"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of XLMRobertaforTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "#setting up a device for PyTorch and loading a pre-trained XLM-Roberta model for token classification. checks if a CUDA-compatible GPU is available on the system. If it is, it sets the device to GPU (cuda); otherwise, it sets it to CPU.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "xlmr_my_model = (XLMRobertaforTokenClassification.from_pretrained(xlmr_model_name, config = xlmr_config).to(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LHVoL5JuxcI",
        "outputId": "94f84bd1-1455-4e3f-e5f9-548044f13c90"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<s>',\n",
              " '▁My',\n",
              " '▁leg',\n",
              " 's',\n",
              " '▁are',\n",
              " '▁in',\n",
              " '▁pain',\n",
              " '▁from',\n",
              " '▁this',\n",
              " '▁weight',\n",
              " '</s>']"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example_string = 'My legs are in pain from this weight'\n",
        "xlmr_tokens = xlmr_tokenizer(example_string).tokens()\n",
        "xlmr_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Vef7KuevHC0"
      },
      "outputs": [],
      "source": [
        "#defining a function to tag text using pretrained model and tokenizer\n",
        "def tag_text(text, tags, model, tokenizer):\n",
        "  #tokenize the text, preserving special characters\n",
        "  tokens = tokenizer(text).tokens()\n",
        "\n",
        "  # Convert tokens to input IDs and move them to the appropriate device\n",
        "  input_ids = tokenizer(text, return_tensors = 'pt').input_ids.to(device)\n",
        "  print('input_ids:', input_ids)\n",
        "\n",
        "  #get the model outputs for the given input ids\n",
        "  outputs = model(input_ids)[0]\n",
        "  print('Shape of Outputs:', outputs.shape)\n",
        "\n",
        "  #argmax over the tag dimension to get the most likely class for each token\n",
        "  predictions = torch.argmax(outputs, dim = 2)\n",
        "  print('Predictions:', predictions)\n",
        "\n",
        "  #mapping predictions to tag names\n",
        "  preds = [tags.names[p] for p in predictions[0].cpu().numpy()]\n",
        "\n",
        "  # Return tokens and their predicted tags as a DataFrame\n",
        "  return pd.DataFrame([tokens, preds], ['Tokens', 'NER Tags'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZm2_d-swPhr",
        "outputId": "1a34f11b-ae8d-4609-a7ed-fced89b979c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], id=None)\n",
            "XLMRobertaTokenizerFast(name_or_path='xlm-roberta-base', vocab_size=250002, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
            "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t250001: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "print(tags)\n",
        "print(xlmr_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "pZbas4cewmP5",
        "outputId": "2f472130-1b95-4a05-f4ee-d94ef370c4a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_ids: tensor([[    0,  2646,  6049,     7,   621,    23, 24503,  1295,   903, 57888,\n",
            "             2]])\n",
            "Shape of Outputs: torch.Size([1, 11, 7])\n",
            "Predictions: tensor([[4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]])\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Tokens</th>\n",
              "      <td>&lt;s&gt;</td>\n",
              "      <td>▁My</td>\n",
              "      <td>▁leg</td>\n",
              "      <td>s</td>\n",
              "      <td>▁are</td>\n",
              "      <td>▁in</td>\n",
              "      <td>▁pain</td>\n",
              "      <td>▁from</td>\n",
              "      <td>▁this</td>\n",
              "      <td>▁weight</td>\n",
              "      <td>&lt;/s&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NER Tags</th>\n",
              "      <td>I-ORG</td>\n",
              "      <td>B-LOC</td>\n",
              "      <td>B-LOC</td>\n",
              "      <td>B-LOC</td>\n",
              "      <td>B-LOC</td>\n",
              "      <td>B-LOC</td>\n",
              "      <td>B-LOC</td>\n",
              "      <td>B-LOC</td>\n",
              "      <td>B-LOC</td>\n",
              "      <td>B-LOC</td>\n",
              "      <td>B-LOC</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             0      1      2      3      4      5      6      7      8   \\\n",
              "Tokens      <s>    ▁My   ▁leg      s   ▁are    ▁in  ▁pain  ▁from  ▁this   \n",
              "NER Tags  I-ORG  B-LOC  B-LOC  B-LOC  B-LOC  B-LOC  B-LOC  B-LOC  B-LOC   \n",
              "\n",
              "               9      10  \n",
              "Tokens    ▁weight   </s>  \n",
              "NER Tags    B-LOC  B-LOC  "
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tag_text(example_string,tags,xlmr_my_model,xlmr_tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEe16Tz1x6BK"
      },
      "source": [
        "Tokenizing texts for ner. we tokenize each word and use the is_split_into_words argument to indicate that our input sequence is already split into words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFqdQRcPyDrM",
        "outputId": "736a7b09-969f-48c9-dfae-9912e892e9bc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'tokens': [\"''\",\n",
              "  'January',\n",
              "  '21',\n",
              "  \"''\",\n",
              "  '–',\n",
              "  'Nanny',\n",
              "  'and',\n",
              "  'the',\n",
              "  'Professor'],\n",
              " 'ner_tags': [0, 0, 0, 0, 0, 1, 2, 2, 2],\n",
              " 'langs': ['en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en'],\n",
              " 'ner_tags_str': ['O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'I-PER']}"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "en_example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JaHl5Tqix1p3"
      },
      "outputs": [],
      "source": [
        "words, labels = en_example['tokens'], en_example['ner_tags']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5uqTCN3yPqK",
        "outputId": "ac3c1d9a-b7b2-470b-a7bd-27329addae29"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0, 0, 0, 0, 0, 1, 2, 2, 2]"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEgBJLDfyUgG",
        "outputId": "b9c96b86-a6e1-495f-fb93-8989cd47f843"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<s>',\n",
              " \"▁''\",\n",
              " '▁January',\n",
              " '▁21',\n",
              " \"▁''\",\n",
              " '▁–',\n",
              " '▁Nan',\n",
              " 'ny',\n",
              " '▁and',\n",
              " '▁the',\n",
              " '▁Professor',\n",
              " '</s>']"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#is_split_into_words DOES NOT MEAN that the text was already pre-tokenized. It just means that the string was split into words (not tokens), i.e., split on spaces.\n",
        "tokenized_input = xlmr_tokenizer(en_example['tokens'],is_split_into_words=True)\n",
        "tokens = xlmr_tokenizer.convert_ids_to_tokens(tokenized_input['input_ids'])\n",
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "IEXnUddAyzS-",
        "outputId": "abb2bc2e-05e0-40bd-e28c-4ebc30d4a2e6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Tokens</th>\n",
              "      <td>&lt;s&gt;</td>\n",
              "      <td>▁''</td>\n",
              "      <td>▁January</td>\n",
              "      <td>▁21</td>\n",
              "      <td>▁''</td>\n",
              "      <td>▁–</td>\n",
              "      <td>▁Nan</td>\n",
              "      <td>ny</td>\n",
              "      <td>▁and</td>\n",
              "      <td>▁the</td>\n",
              "      <td>▁Professor</td>\n",
              "      <td>&lt;/s&gt;</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         0    1         2    3    4   5     6   7     8     9           10  \\\n",
              "Tokens  <s>  ▁''  ▁January  ▁21  ▁''  ▁–  ▁Nan  ny  ▁and  ▁the  ▁Professor   \n",
              "\n",
              "          11  \n",
              "Tokens  </s>  "
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.DataFrame([tokens], index=[\"Tokens\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "XV4Cb56ZzCNx",
        "outputId": "5b55f4cd-bb19-494c-eee9-e43105d6f674"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "word_ids  [None, 0, 1, 2, 3, 4, 5, 5, 6, 7, 8, None]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Tokens</th>\n",
              "      <td>&lt;s&gt;</td>\n",
              "      <td>▁''</td>\n",
              "      <td>▁January</td>\n",
              "      <td>▁21</td>\n",
              "      <td>▁''</td>\n",
              "      <td>▁–</td>\n",
              "      <td>▁Nan</td>\n",
              "      <td>ny</td>\n",
              "      <td>▁and</td>\n",
              "      <td>▁the</td>\n",
              "      <td>▁Professor</td>\n",
              "      <td>&lt;/s&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Word IDs</th>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            0    1         2    3    4   5     6   7     8     9           10  \\\n",
              "Tokens     <s>  ▁''  ▁January  ▁21  ▁''  ▁–  ▁Nan  ny  ▁and  ▁the  ▁Professor   \n",
              "Word IDs  None    0         1    2    3   4     5   5     6     7           8   \n",
              "\n",
              "            11  \n",
              "Tokens    </s>  \n",
              "Word IDs  None  "
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "word_ids = tokenized_input.word_ids() # Get the word IDs from the tokenized input\n",
        "print('word_ids ', word_ids)\n",
        "\n",
        "pd.DataFrame([tokens, word_ids], index=[\"Tokens\", \"Word IDs\"]) # Create a DataFrame to display tokens along with their corresponding word IDs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "8iVSQVT6zMZ2",
        "outputId": "3b802af2-f68e-4efe-cbae-4ee1299a2c83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label_ids  [-100, 0, 0, 0, 0, 0, 1, -100, 2, 2, 2, -100]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Tokens</th>\n",
              "      <td>&lt;s&gt;</td>\n",
              "      <td>▁''</td>\n",
              "      <td>▁January</td>\n",
              "      <td>▁21</td>\n",
              "      <td>▁''</td>\n",
              "      <td>▁–</td>\n",
              "      <td>▁Nan</td>\n",
              "      <td>ny</td>\n",
              "      <td>▁and</td>\n",
              "      <td>▁the</td>\n",
              "      <td>▁Professor</td>\n",
              "      <td>&lt;/s&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Word IDs</th>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Label IDs</th>\n",
              "      <td>-100</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-100</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>-100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Labels</th>\n",
              "      <td>IGN</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>B-PER</td>\n",
              "      <td>IGN</td>\n",
              "      <td>I-PER</td>\n",
              "      <td>I-PER</td>\n",
              "      <td>I-PER</td>\n",
              "      <td>IGN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             0    1         2    3    4   5      6     7      8      9   \\\n",
              "Tokens      <s>  ▁''  ▁January  ▁21  ▁''  ▁–   ▁Nan    ny   ▁and   ▁the   \n",
              "Word IDs   None    0         1    2    3   4      5     5      6      7   \n",
              "Label IDs  -100    0         0    0    0   0      1  -100      2      2   \n",
              "Labels      IGN    O         O    O    O   O  B-PER   IGN  I-PER  I-PER   \n",
              "\n",
              "                   10    11  \n",
              "Tokens     ▁Professor  </s>  \n",
              "Word IDs            8  None  \n",
              "Label IDs           2  -100  \n",
              "Labels          I-PER   IGN  "
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "previous_word_idx = None\n",
        "label_ids = []\n",
        "\n",
        "for word_idx in word_ids:\n",
        "    if word_idx is None or word_idx == previous_word_idx:\n",
        "        label_ids.append(-100) #usind -100 for tokens not associated with a word index\n",
        "    elif word_idx != previous_word_idx:\n",
        "        label_ids.append(labels[word_idx])\n",
        "    # Update previous_word_idx to the current word index\n",
        "    previous_word_idx = word_idx\n",
        "\n",
        "print('label_ids ', label_ids)\n",
        "\n",
        "#converting label IDs to actual labels or \"IGN\" for ignored tokens\n",
        "labels = [index2tag[l] if l != -100 else \"IGN\" for l in label_ids]\n",
        "index = [\"Tokens\", \"Word IDs\", \"Label IDs\", \"Labels\"]\n",
        "\n",
        "pd.DataFrame([tokens, word_ids, label_ids, labels], index=index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vv7NFVuRzzLW"
      },
      "outputs": [],
      "source": [
        "#function to tokenize the input text, mask certain tokens, and adjust the labels accordingly\n",
        "def tokenize_mask_modify_labels(examples):\n",
        "    tokenized_inputs = xlmr_tokenizer(examples[\"tokens\"], truncation=True,\n",
        "                                      is_split_into_words=True)\n",
        "    labels = []\n",
        "    for idx, label in enumerate(examples[\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=idx)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None or word_idx == previous_word_idx:\n",
        "                label_ids.append(-100)\n",
        "            else:\n",
        "                label_ids.append(label[word_idx])\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Map\n",
        "Some of the more powerful applications of huggingface Datasets come from using the map() function. The primary purpose of map() is to speed up processing functions. It allows you to apply a processing function to each example in a dataset, independently or in batches. This function can even create new rows and columns."
      ],
      "metadata": {
        "id": "tYiGV7tlS1Kz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6z3mOAMz7e3"
      },
      "outputs": [],
      "source": [
        "def encode_panx_dataset(corpus):\n",
        "    \"\"\"\n",
        "    Encode a dataset in-place by tokenizing, masking, and modifying labels.\n",
        "\n",
        "    Args:\n",
        "    - corpus (datasets.Dataset): Dataset to be processed, containing columns 'langs', 'ner_tags', and 'tokens'.\n",
        "\n",
        "    Returns:\n",
        "    - datasets.Dataset: Processed dataset with tokenized inputs and modified labels, columns 'input_ids', 'attention_mask', and 'labels'.\n",
        "    \"\"\"\n",
        "    # Use the map method to apply tokenize_mask_modify_labels function to each batch\n",
        "    # of the dataset, removing columns 'langs', 'ner_tags', and 'tokens' in the process.\n",
        "    return corpus.map(tokenize_mask_modify_labels, batched=True, remove_columns=['langs', 'ner_tags', 'tokens'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "ffd2e047db9343f0b7c7d135291f2d27",
            "3ab13f1392f94356b31ae312335ff9a0",
            "8e7afadeddf74d699dd5923e4603e550",
            "73c11a9d954d4747a0c9045803226d2f",
            "9bcbe32efd1642d88dc9171d4fd7450d",
            "7d44845bdded4bac88dd0a06fe7f5692",
            "d633f3e5dea9488e8d24622d75ae0da4",
            "eb251395fd324734802de9dfccabdaf0",
            "e4355bf063b14f4a93c00f23aa1b998d",
            "e67c714357bc43c79249f1a2fd019b2f",
            "7933b7211a1648bba3a24405ebbdd0ea"
          ]
        },
        "id": "mMCNlDzv0A77",
        "outputId": "9357e1b3-1591-4614-dfdb-8cf7aba8b78f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 3258/3258 [00:00<00:00, 17046.72 examples/s]\n"
          ]
        }
      ],
      "source": [
        "panx_en_encoded = encode_panx_dataset(panx_ds_combined[\"en\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZ7pUSUx0G8v",
        "outputId": "6f4db95a-bc71-4c64-89b6-36fb81e774de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "        MISC       0.00      0.00      0.00         1\n",
            "         PER       1.00      1.00      1.00         1\n",
            "\n",
            "   micro avg       0.50      0.50      0.50         2\n",
            "   macro avg       0.50      0.50      0.50         2\n",
            "weighted avg       0.50      0.50      0.50         2\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Importing the classification_report function from seqeval.metrics\n",
        "from seqeval.metrics import classification_report\n",
        "\n",
        "# True labels (ground truth) for two sequences\n",
        "y_true = [[\"O\", \"O\", \"O\", \"B-MISC\", \"I-MISC\", \"I-MISC\", \"O\"],\n",
        "          [\"B-PER\", \"I-PER\", \"O\"]]\n",
        "\n",
        "# Predicted labels for the same sequences as y_true\n",
        "y_pred = [[\"O\", \"O\", \"B-MISC\", \"I-MISC\", \"I-MISC\", \"I-MISC\", \"O\"],\n",
        "          [\"B-PER\", \"I-PER\", \"O\"]]\n",
        "\n",
        "# Generating and printing the classification report comparing y_true and y_pred\n",
        "print(classification_report(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TASy7z-suFuL"
      },
      "outputs": [],
      "source": [
        "# eval prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOUEk29DuNu5"
      },
      "outputs": [],
      "source": [
        "def generate_list_compute_metrics(predictions, label_ids):\n",
        "    # predictions: predicted probabilities or logits (output of the model)\n",
        "    # label_ids: true label indices for each token in each example\n",
        "\n",
        "    # Get the predicted labels by taking the argmax along the last axis (which represents classes)\n",
        "  preds = np.argmax(predictions, axis=2)\n",
        "  batch_size, seq_len = preds.shape\n",
        "  predictions_list, true_list = [],[]\n",
        "\n",
        "  for b_idx in range(batch_size): #iterating over each example in the batch\n",
        "    example_labels, example_preds = [],[]\n",
        "    for s_idx in range(seq_len): #iterating over each token in the sequence\n",
        "      if label_ids[b_idx][s_idx] != -100: #ignoring masked tokens\n",
        "        example_preds.append(index2tag[preds[b_idx][s_idx]])\n",
        "        example_labels.append(index2tag[label_ids[b_idx][s_idx]])\n",
        "    predictions_list.append(example_preds)\n",
        "    true_list.append(example_labels)\n",
        "  return predictions_list, true_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1BX1kfMMZ09",
        "outputId": "09e00ffb-0489-42ee-aeec-f93765df00e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: accelerate in c:\\python312\\lib\\site-packages (0.31.0)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\python312\\lib\\site-packages (from accelerate) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\python312\\lib\\site-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in c:\\users\\banga\\appdata\\roaming\\python\\python312\\site-packages (from accelerate) (5.9.8)\n",
            "Requirement already satisfied: pyyaml in c:\\python312\\lib\\site-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in c:\\python312\\lib\\site-packages (from accelerate) (2.3.1)\n",
            "Requirement already satisfied: huggingface-hub in c:\\python312\\lib\\site-packages (from accelerate) (0.23.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in c:\\python312\\lib\\site-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in c:\\python312\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\python312\\lib\\site-packages (from torch>=1.10.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: sympy in c:\\python312\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.12.1)\n",
            "Requirement already satisfied: networkx in c:\\python312\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in c:\\python312\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: fsspec in c:\\python312\\lib\\site-packages (from torch>=1.10.0->accelerate) (2024.3.1)\n",
            "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\python312\\lib\\site-packages (from torch>=1.10.0->accelerate) (2021.4.0)\n",
            "Requirement already satisfied: requests in c:\\python312\\lib\\site-packages (from huggingface-hub->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in c:\\python312\\lib\\site-packages (from huggingface-hub->accelerate) (4.66.4)\n",
            "Requirement already satisfied: intel-openmp==2021.* in c:\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.10.0->accelerate) (2021.4.0)\n",
            "Requirement already satisfied: tbb==2021.* in c:\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.10.0->accelerate) (2021.12.0)\n",
            "Requirement already satisfied: colorama in c:\\python312\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub->accelerate) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python312\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python312\\lib\\site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\python312\\lib\\site-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python312\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\python312\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in c:\\python312\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZwIl_fTvmhG",
        "outputId": "1c8ab315-4212-4d37-fcd6-4e07234305b1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 3\n",
        "batch_size = 24\n",
        "logging_steps = len(panx_en_encoded[\"train\"]) // batch_size\n",
        "model_name = f\"{xlmr_model_name}-finetuned on panx english\"\n",
        "\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=model_name,                    # Directory where model checkpoints and logs will be saved\n",
        "    log_level=\"error\",                        # Set log level (error, warning, info, debug)\n",
        "    num_train_epochs=num_epochs,              # Number of training epochs\n",
        "    per_device_train_batch_size=batch_size,   # Batch size per GPU/TPU core for training\n",
        "    per_device_eval_batch_size=batch_size,    # Batch size per GPU/TPU core for evaluation\n",
        "    evaluation_strategy=\"epoch\",              # Evaluate every epoch\n",
        "    save_steps=1e6,                           # Number of steps before saving model checkpoint\n",
        "    weight_decay=0.01,                        # Weight decay for regularization\n",
        "    disable_tqdm=False,                       # Disable tqdm progress bars\n",
        "    push_to_hub=False,                        # Whether to push to the Hub (if using Transformers Hub)\n",
        "    logging_steps=logging_steps,              # Log metrics every `logging_steps` steps\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lPh__4FwaHH"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(p: EvalPrediction):\n",
        "  print('eval prediction', p.predictions)\n",
        "  y_pred,y_true = generate_list_compute_metrics(p.predictions, p.label_ids)\n",
        "  return{\"f1\": f1_score(y_true,y_pred)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCRINwB2zHRY"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForTokenClassification(xlmr_tokenizer) #initializing DataCollatorForTokenClassification with XLM-RoBERTa tokenizer\n",
        "\n",
        "def model_init(): #initialize the token classification model\n",
        "  return (XLMRobertaforTokenClassification.from_pretrained(xlmr_model_name, config = xlmr_config).to(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tuning XLMRoberta"
      ],
      "metadata": {
        "id": "PSbPMK-LU4Jc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xh_ySw4izb6r"
      },
      "outputs": [],
      "source": [
        "#fine tuning\n",
        "\n",
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model_init = model_init,\n",
        "    args = training_args,\n",
        "    data_collator = data_collator,\n",
        "    compute_metrics = compute_metrics,\n",
        "    train_dataset = panx_en_encoded[\"train\"],\n",
        "    eval_dataset = panx_en_encoded[\"test\"],\n",
        "    tokenizer = xlmr_tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "YPF03hwIz3BY",
        "outputId": "be873677-f3f9-4e12-97bd-c4e2cfedbdf7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 33%|███▎      | 271/816 [25:02<46:19,  5.10s/it]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.5416, 'grad_norm': 8.992460250854492, 'learning_rate': 3.339460784313725e-05, 'epoch': 1.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 33%|███▎      | 272/816 [25:05<40:15,  4.44s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "eval prediction [[[ 1.61168385e+00  1.93553358e-01 -8.20245504e-01 ...  2.82754228e-02\n",
            "    3.04702103e-01 -1.13533211e+00]\n",
            "  [ 1.94839060e+00  4.59912419e-01 -2.37556744e+00 ... -1.21153152e+00\n",
            "    1.54391217e+00 -2.16635203e+00]\n",
            "  [ 2.65918159e+00 -2.46978331e+00 -2.43305564e-02 ...  2.75960350e+00\n",
            "   -2.23649883e+00  5.03501534e-01]\n",
            "  ...\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]]\n",
            "\n",
            " [[ 8.16402674e-01 -1.61698431e-01 -6.89865127e-02 ...  2.16685236e-03\n",
            "   -2.33492002e-01 -2.14633495e-01]\n",
            "  [-5.39389372e-01  5.62044525e+00 -1.83847201e+00 ... -1.93583179e+00\n",
            "    1.98128626e-01 -3.60505939e+00]\n",
            "  [-1.14877224e+00 -1.09169114e+00  5.25410271e+00 ...  2.47328043e+00\n",
            "   -2.00852394e+00  3.03800404e-03]\n",
            "  ...\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]]\n",
            "\n",
            " [[ 1.05434585e+00 -1.39996335e-01 -1.64071620e-01 ... -9.08429846e-02\n",
            "   -2.36290812e-01 -2.78255761e-01]\n",
            "  [ 6.92129993e+00 -1.15692544e+00 -1.73843336e+00 ... -1.46340609e+00\n",
            "   -6.00647211e-01 -1.98377919e+00]\n",
            "  [ 6.55843639e+00 -1.43685865e+00 -1.68162990e+00 ... -1.06675494e+00\n",
            "   -9.16793466e-01 -1.41158772e+00]\n",
            "  ...\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 7.26582825e-01 -2.12255403e-01 -8.52380276e-01 ...  9.01504517e-01\n",
            "   -1.06983036e-01 -9.65560675e-01]\n",
            "  [-6.42711341e-01  1.71583903e+00 -2.27267933e+00 ... -1.01331782e+00\n",
            "    1.89788032e+00 -2.90109062e+00]\n",
            "  [-5.74066281e-01 -1.54670942e+00  8.46144676e-01 ...  3.40091658e+00\n",
            "   -1.74011588e+00  1.47939444e+00]\n",
            "  ...\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]]\n",
            "\n",
            " [[-8.60190392e-02 -1.15196538e+00 -1.85709572e+00 ... -7.72938207e-02\n",
            "    1.47642851e+00  1.40329075e+00]\n",
            "  [-1.50405645e+00 -1.31316036e-01 -3.64503479e+00 ... -1.43034053e+00\n",
            "    5.14946175e+00 -6.30852282e-01]\n",
            "  [-8.59814167e-01 -3.58400393e+00 -2.96210647e-01 ...  2.66060162e+00\n",
            "   -3.46534520e-01  5.33555460e+00]\n",
            "  ...\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]]\n",
            "\n",
            " [[ 1.28262281e+00  1.07121244e-02  1.71390176e-03 ... -1.61794379e-01\n",
            "   -2.71823406e-01 -5.43852925e-01]\n",
            "  [-1.06720304e+00  5.58799744e+00 -1.14038897e+00 ... -1.79815555e+00\n",
            "   -3.30207735e-01 -3.39645982e+00]\n",
            "  [-1.54080606e+00 -1.20030153e+00  5.63636017e+00 ...  2.29793262e+00\n",
            "   -1.90968013e+00 -6.70313686e-02]\n",
            "  ...\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \n",
            " 33%|███▎      | 272/816 [27:35<40:15,  4.44s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.3296959400177002, 'eval_f1': 0.7362115265441024, 'eval_runtime': 149.718, 'eval_samples_per_second': 21.761, 'eval_steps_per_second': 0.908, 'epoch': 1.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 66%|██████▋   | 542/816 [51:54<39:55,  8.74s/it]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2954, 'grad_norm': 4.983207702636719, 'learning_rate': 1.678921568627451e-05, 'epoch': 1.99}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 67%|██████▋   | 544/816 [52:03<28:52,  6.37s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "eval prediction [[[ 2.1051674e+00  1.6002744e-02 -4.5070237e-01 ...  4.1323621e-02\n",
            "   -2.3694363e-01 -1.1207001e+00]\n",
            "  [ 4.0705748e+00  7.1567631e-01 -2.6463609e+00 ... -1.7420857e+00\n",
            "    1.2597938e+00 -3.5140505e+00]\n",
            "  [ 4.7437778e+00 -2.6340892e+00  2.7744853e-01 ...  2.2443707e+00\n",
            "   -2.7068174e+00 -6.0830522e-01]\n",
            "  ...\n",
            "  [-1.0000000e+02 -1.0000000e+02 -1.0000000e+02 ... -1.0000000e+02\n",
            "   -1.0000000e+02 -1.0000000e+02]\n",
            "  [-1.0000000e+02 -1.0000000e+02 -1.0000000e+02 ... -1.0000000e+02\n",
            "   -1.0000000e+02 -1.0000000e+02]\n",
            "  [-1.0000000e+02 -1.0000000e+02 -1.0000000e+02 ... -1.0000000e+02\n",
            "   -1.0000000e+02 -1.0000000e+02]]\n",
            "\n",
            " [[ 2.0466490e+00 -5.2606082e-01 -2.5325751e-01 ...  4.6759915e-01\n",
            "   -5.1273954e-01 -9.0093946e-01]\n",
            "  [ 8.2598329e-03  5.6770787e+00 -1.6933768e+00 ... -1.7214979e+00\n",
            "   -1.8391693e-01 -3.7484708e+00]\n",
            "  [-3.5606807e-01 -7.6042694e-01  4.5297313e+00 ...  1.8277693e+00\n",
            "   -1.9076734e+00 -5.1208615e-01]\n",
            "  ...\n",
            "  [-1.0000000e+02 -1.0000000e+02 -1.0000000e+02 ... -1.0000000e+02\n",
            "   -1.0000000e+02 -1.0000000e+02]\n",
            "  [-1.0000000e+02 -1.0000000e+02 -1.0000000e+02 ... -1.0000000e+02\n",
            "   -1.0000000e+02 -1.0000000e+02]\n",
            "  [-1.0000000e+02 -1.0000000e+02 -1.0000000e+02 ... -1.0000000e+02\n",
            "   -1.0000000e+02 -1.0000000e+02]]\n",
            "\n",
            " [[ 2.0859776e+00 -3.0486235e-01 -3.2087681e-01 ...  5.5307209e-02\n",
            "   -4.3286839e-01 -6.7962849e-01]\n",
            "  [ 7.5520868e+00 -1.2172948e+00 -1.2090465e+00 ... -1.4188548e+00\n",
            "   -8.6539888e-01 -2.2328458e+00]\n",
            "  [ 7.5208459e+00 -1.6258874e+00 -1.2794025e+00 ... -1.0466825e+00\n",
            "   -1.0769119e+00 -1.9360176e+00]\n",
            "  ...\n",
            "  [-1.0000000e+02 -1.0000000e+02 -1.0000000e+02 ... -1.0000000e+02\n",
            "   -1.0000000e+02 -1.0000000e+02]\n",
            "  [-1.0000000e+02 -1.0000000e+02 -1.0000000e+02 ... -1.0000000e+02\n",
            "   -1.0000000e+02 -1.0000000e+02]\n",
            "  [-1.0000000e+02 -1.0000000e+02 -1.0000000e+02 ... -1.0000000e+02\n",
            "   -1.0000000e+02 -1.0000000e+02]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 2.9055399e-01 -1.1088247e+00 -3.9826730e-01 ...  1.9471624e+00\n",
            "   -6.8071008e-01  7.0518456e-02]\n",
            "  [-1.1859689e+00  1.9990938e+00 -2.4478595e+00 ... -8.1261784e-01\n",
            "    2.3409402e+00 -3.2279987e+00]\n",
            "  [-1.3982069e+00 -2.0592256e+00  1.3572444e+00 ...  4.6138773e+00\n",
            "   -1.9769441e+00  2.1172113e+00]\n",
            "  ...\n",
            "  [-1.0000000e+02 -1.0000000e+02 -1.0000000e+02 ... -1.0000000e+02\n",
            "   -1.0000000e+02 -1.0000000e+02]\n",
            "  [-1.0000000e+02 -1.0000000e+02 -1.0000000e+02 ... -1.0000000e+02\n",
            "   -1.0000000e+02 -1.0000000e+02]\n",
            "  [-1.0000000e+02 -1.0000000e+02 -1.0000000e+02 ... -1.0000000e+02\n",
            "   -1.0000000e+02 -1.0000000e+02]]\n",
            "\n",
            " [[ 2.5016141e-01 -1.7884727e+00 -1.6781696e+00 ...  3.4969229e-01\n",
            "    1.1661712e+00  2.1257789e+00]\n",
            "  [-1.5688164e+00 -3.8101375e-01 -3.7620337e+00 ... -1.0250421e+00\n",
            "    5.3974848e+00 -9.0548497e-01]\n",
            "  [-3.4562802e-01 -3.6301637e+00  1.2598589e-01 ...  3.0369263e+00\n",
            "   -9.6429205e-01  5.3187132e+00]\n",
            "  ...\n",
            "  [-1.0000000e+02 -1.0000000e+02 -1.0000000e+02 ... -1.0000000e+02\n",
            "   -1.0000000e+02 -1.0000000e+02]\n",
            "  [-1.0000000e+02 -1.0000000e+02 -1.0000000e+02 ... -1.0000000e+02\n",
            "   -1.0000000e+02 -1.0000000e+02]\n",
            "  [-1.0000000e+02 -1.0000000e+02 -1.0000000e+02 ... -1.0000000e+02\n",
            "   -1.0000000e+02 -1.0000000e+02]]\n",
            "\n",
            " [[ 3.3293827e+00  5.2843809e-02  1.1024660e-01 ... -2.1175557e-01\n",
            "   -6.5567017e-01 -1.5531178e+00]\n",
            "  [-1.1429355e+00  5.7696052e+00 -1.0597124e+00 ... -1.3311348e+00\n",
            "   -4.9558377e-01 -3.4679596e+00]\n",
            "  [-1.4316666e+00 -9.7677588e-01  5.1323261e+00 ...  2.1233485e+00\n",
            "   -1.7628028e+00 -2.8681585e-01]\n",
            "  ...\n",
            "  [-1.0000000e+02 -1.0000000e+02 -1.0000000e+02 ... -1.0000000e+02\n",
            "   -1.0000000e+02 -1.0000000e+02]\n",
            "  [-1.0000000e+02 -1.0000000e+02 -1.0000000e+02 ... -1.0000000e+02\n",
            "   -1.0000000e+02 -1.0000000e+02]\n",
            "  [-1.0000000e+02 -1.0000000e+02 -1.0000000e+02 ... -1.0000000e+02\n",
            "   -1.0000000e+02 -1.0000000e+02]]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \n",
            " 67%|██████▋   | 544/816 [54:33<28:52,  6.37s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.28713440895080566, 'eval_f1': 0.772178699978827, 'eval_runtime': 150.7495, 'eval_samples_per_second': 21.612, 'eval_steps_per_second': 0.902, 'epoch': 2.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 813/816 [1:20:49<00:15,  5.15s/it]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2097, 'grad_norm': 7.200422763824463, 'learning_rate': 1.8382352941176472e-07, 'epoch': 2.99}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 816/816 [1:21:01<00:00,  4.40s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "eval prediction [[[ 1.46049130e+00 -3.71023789e-02 -5.66521764e-01 ... -6.71732128e-02\n",
            "    9.17637199e-02 -7.39446223e-01]\n",
            "  [ 3.83629084e+00  3.14959109e-01 -3.00060081e+00 ... -2.05165267e+00\n",
            "    1.74679971e+00 -3.00509357e+00]\n",
            "  [ 4.89869308e+00 -3.02554798e+00 -6.37388110e-01 ...  1.96551132e+00\n",
            "   -2.31688643e+00 -1.62800461e-01]\n",
            "  ...\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]]\n",
            "\n",
            " [[ 1.82026315e+00 -3.67643476e-01 -2.34368905e-01 ...  1.25174522e-01\n",
            "   -4.47949737e-01 -7.34403849e-01]\n",
            "  [ 2.94063449e-01  6.40595722e+00 -1.79854214e+00 ... -2.13840580e+00\n",
            "   -7.60737360e-01 -4.04679489e+00]\n",
            "  [-1.01316355e-01 -9.64817047e-01  6.11404943e+00 ...  2.02791357e+00\n",
            "   -2.63783097e+00 -1.15156198e+00]\n",
            "  ...\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]]\n",
            "\n",
            " [[ 2.23778057e+00 -3.14572543e-01 -4.15403843e-01 ... -6.13554567e-02\n",
            "   -4.02813435e-01 -6.76942348e-01]\n",
            "  [ 8.09427261e+00 -1.51089776e+00 -1.53690505e+00 ... -1.54056692e+00\n",
            "   -9.80376840e-01 -1.93256569e+00]\n",
            "  [ 7.76963949e+00 -1.77143025e+00 -1.66512692e+00 ... -1.27286983e+00\n",
            "   -9.96338427e-01 -1.75742507e+00]\n",
            "  ...\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-3.57476860e-01 -9.70112562e-01 -4.52579916e-01 ...  1.88841093e+00\n",
            "   -1.28324300e-01 -2.43390948e-02]\n",
            "  [-1.25686288e+00  2.12611198e+00 -2.92627954e+00 ... -1.43847847e+00\n",
            "    2.74781489e+00 -3.58903027e+00]\n",
            "  [-1.93081546e+00 -2.41510415e+00  1.51375961e+00 ...  4.92146301e+00\n",
            "   -1.98546267e+00  2.58104992e+00]\n",
            "  ...\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]]\n",
            "\n",
            " [[ 3.43502283e-01 -1.73096883e+00 -1.78564000e+00 ...  4.16196346e-01\n",
            "    1.05240369e+00  2.04687738e+00]\n",
            "  [-1.54954016e+00 -6.80414379e-01 -4.21477079e+00 ... -1.12671113e+00\n",
            "    5.91027355e+00 -8.81646037e-01]\n",
            "  [-3.70983183e-01 -3.92117167e+00 -2.99652129e-01 ...  3.35740566e+00\n",
            "   -1.06859004e+00  5.68417883e+00]\n",
            "  ...\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]]\n",
            "\n",
            " [[ 3.39677954e+00  2.40862146e-02  1.47481740e-01 ... -4.41187620e-01\n",
            "   -6.48525715e-01 -1.39307666e+00]\n",
            "  [-1.05942047e+00  6.59737778e+00 -8.44515204e-01 ... -1.67699957e+00\n",
            "   -1.19814467e+00 -3.76063299e+00]\n",
            "  [-1.56230330e+00 -1.02022326e+00  6.58587265e+00 ...  2.10254121e+00\n",
            "   -2.18271685e+00 -1.01213408e+00]\n",
            "  ...\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                   \n",
            "100%|██████████| 816/816 [1:23:38<00:00,  6.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.28108614683151245, 'eval_f1': 0.7790427999156653, 'eval_runtime': 157.1895, 'eval_samples_per_second': 20.727, 'eval_steps_per_second': 0.865, 'epoch': 3.0}\n",
            "{'train_runtime': 5018.9215, 'train_samples_per_second': 3.895, 'train_steps_per_second': 0.163, 'train_loss': 0.3487163671091491, 'epoch': 3.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=816, training_loss=0.3487163671091491, metrics={'train_runtime': 5018.9215, 'train_samples_per_second': 3.895, 'train_steps_per_second': 0.163, 'total_flos': 394314629981184.0, 'train_loss': 0.3487163671091491, 'epoch': 3.0})"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqRIuP6SzwvX",
        "outputId": "b764e2f7-41db-4753-d90e-563e0ab995e9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5416</td>\n",
              "      <td>0.329696</td>\n",
              "      <td>0.736212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.2954</td>\n",
              "      <td>0.287134</td>\n",
              "      <td>0.772179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>0.2097</td>\n",
              "      <td>0.281086</td>\n",
              "      <td>0.779043</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Epoch  Training Loss  Validation Loss        F1\n",
              "0      1         0.5416         0.329696  0.736212\n",
              "2      2         0.2954         0.287134  0.772179\n",
              "4      3         0.2097         0.281086  0.779043"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.DataFrame(trainer.state.log_history)[['epoch','loss','eval_loss','eval_f1']]\n",
        "df = df.rename(columns={'epoch':'Epoch','loss':'Training Loss','eval_loss':'Validation Loss', 'eval_f1':'F1'})\n",
        "df['Epoch'] = df['Epoch'].apply(lambda x: round(x))\n",
        "df['Training Loss'] = df['Training Loss'].ffill()\n",
        "df[['Validation Loss', 'F1']] = df[['Validation Loss', 'F1']].bfill().ffill()\n",
        "df.drop_duplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8xdBpnbMZ0_",
        "outputId": "a953a68c-a968-4497-8ba3-6a0db38ee6f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_ids: tensor([[     0,   4939,     83, 150080, 102126,    903,  51065,      2]])\n",
            "Shape of Outputs: torch.Size([1, 8, 7])\n",
            "Predictions: tensor([[0, 1, 0, 0, 5, 0, 0, 0]])\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Tokens</th>\n",
              "      <td>&lt;s&gt;</td>\n",
              "      <td>▁John</td>\n",
              "      <td>▁is</td>\n",
              "      <td>▁visiting</td>\n",
              "      <td>▁Germany</td>\n",
              "      <td>▁this</td>\n",
              "      <td>▁summer</td>\n",
              "      <td>&lt;/s&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NER Tags</th>\n",
              "      <td>O</td>\n",
              "      <td>B-PER</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>B-LOC</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            0      1    2          3         4      5        6     7\n",
              "Tokens    <s>  ▁John  ▁is  ▁visiting  ▁Germany  ▁this  ▁summer  </s>\n",
              "NER Tags    O  B-PER    O          O     B-LOC      O        O     O"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_en = \"John is visiting Germany this summer\"\n",
        "tag_text(text_en, tags, trainer.model, xlmr_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TN7e6eIOMZ0_",
        "outputId": "a4f873cb-a35f-4b4a-c111-fa4fb743b6b6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'attention_mask', 'labels'],\n",
              "    num_rows: 3258\n",
              "})"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "valid_set_batch = panx_en_encoded[\"validation\"]\n",
        "valid_set_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ce-WUA7MZ0_",
        "outputId": "ad133124-6b61-469a-99b3-bc95693ef75b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
              " 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\n",
              " 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "valid_set_batch.features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zp9nFFzUMZ0_",
        "outputId": "28706d79-bb76-4995-835e-ef44563f91bc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None)"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "valid_set_batch.features[\"input_ids\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wi2AvPowMZ0_",
        "outputId": "5be56040-e6c0-4311-ec4a-ba7882a5fb0e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "valid_set_batch.features['attention_mask']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLvLnMfhMZ0_"
      },
      "outputs": [],
      "source": [
        "from torch.nn.functional import cross_entropy\n",
        "\n",
        "def forward_loss_labels(batch):\n",
        "    # Create features dictionary for batch\n",
        "    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
        "\n",
        "    # Use data_collator to prepare batch\n",
        "    batch = data_collator(features)\n",
        "\n",
        "    # Move input_ids, attention_mask, and labels to device\n",
        "    input_ids = batch[\"input_ids\"].to(device)\n",
        "    attention_mask = batch[\"attention_mask\"].to(device)\n",
        "    labels = batch[\"labels\"].to(device)\n",
        "\n",
        "    # Disable gradient computation for inference\n",
        "    with torch.no_grad():\n",
        "        # Forward pass through the model\n",
        "        output = trainer.model(input_ids, attention_mask)\n",
        "        # Predicted labels (indices with maximum probability)\n",
        "        predicted_label = torch.argmax(output.logits, axis=-1).cpu().numpy()\n",
        "\n",
        "    # Calculate cross-entropy loss\n",
        "    loss = cross_entropy(output.logits.view(-1, 7), labels.view(-1), reduction=\"none\")\n",
        "    # Reshape loss to match batch size\n",
        "    loss = loss.view(len(input_ids), -1).cpu().numpy()\n",
        "\n",
        "    # Return dictionary with loss and predicted labels\n",
        "    return {\"loss\": loss, \"predicted_label\": predicted_label}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ff52q-vLMZ1A",
        "outputId": "088a4c7e-ad5c-4bd3-c863-53c7a81fc970"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 3258/3258 [02:33<00:00, 21.23 examples/s]\n"
          ]
        }
      ],
      "source": [
        "valid_set_with_loss = valid_set_batch.map(forward_loss_labels, batched=True, batch_size=32)\n",
        "df = valid_set_with_loss.to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPJlaxd2MZ1A",
        "outputId": "6c42a42e-128b-4069-fe4e-c02b0a1fe29d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3258, 5)"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alzQP-TPMZ1A",
        "outputId": "629756a0-d4b3-45d5-c7b5-18e423861540"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of                                               input_ids  \\\n",
              "0     [0, 5106, 235474, 14, 15491, 15619, 152, 106, ...   \n",
              "1     [0, 353, 3459, 26708, 13, 78833, 5106, 339, 5,...   \n",
              "2     [0, 4687, 1902, 39395, 5470, 678, 40137, 2548,...   \n",
              "3     [0, 38348, 59338, 15, 1735, 38662, 192859, 138...   \n",
              "4     [0, 636, 330, 122807, 242, 7, 314, 60635, 31, ...   \n",
              "...                                                 ...   \n",
              "3253  [0, 242, 5106, 6, 22905, 724, 5106, 242, 20, 5...   \n",
              "3254  [0, 360, 7270, 6, 4, 764, 435, 678, 8055, 13, ...   \n",
              "3255  [0, 242, 5106, 13684, 48585, 15819, 41975, 510...   \n",
              "3256            [0, 57035, 38026, 6, 4, 23213, 1760, 2]   \n",
              "3257            [0, 26521, 19175, 159, 157, 685, 56, 2]   \n",
              "\n",
              "                                         attention_mask  \\\n",
              "0                     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
              "1               [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
              "2     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
              "3                           [1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
              "4     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
              "...                                                 ...   \n",
              "3253            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
              "3254  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
              "3255                     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
              "3256                           [1, 1, 1, 1, 1, 1, 1, 1]   \n",
              "3257                           [1, 1, 1, 1, 1, 1, 1, 1]   \n",
              "\n",
              "                                                 labels  \\\n",
              "0            [-100, 0, 3, -100, 4, 4, 0, 0, 0, 0, -100]   \n",
              "1     [-100, 5, -100, 6, -100, -100, 0, 0, -100, -10...   \n",
              "2     [-100, 0, 0, 0, 0, 0, 1, 2, -100, 0, 0, 0, -10...   \n",
              "3                  [-100, 5, 6, 6, 6, -100, 6, 6, -100]   \n",
              "4     [-100, 5, -100, -100, 6, -100, 6, -100, -100, ...   \n",
              "...                                                 ...   \n",
              "3253  [-100, 0, 0, 0, -100, -100, 0, 0, 0, 5, -100, ...   \n",
              "3254  [-100, 0, 5, 0, -100, 0, 0, 0, 1, -100, 2, 0, ...   \n",
              "3255         [-100, 0, 0, 1, -100, 2, -100, 0, 0, -100]   \n",
              "3256               [-100, 5, 6, 6, -100, 6, -100, -100]   \n",
              "3257            [-100, 3, 4, 4, -100, -100, -100, -100]   \n",
              "\n",
              "                                                  loss:  \\\n",
              "0     [0.0, 0.0007576456, 0.29892322, 0.0, 0.1764177...   \n",
              "1     [0.0, 0.02863326, 0.0, 0.017999288, 0.0, 0.0, ...   \n",
              "2     [0.0, 0.00046123358, 0.00040105882, 0.00064030...   \n",
              "3     [0.0, 0.034111448, 0.025335532, 0.061247345, 0...   \n",
              "4     [0.0, 0.050668683, 0.0, 0.0, 0.055195283, 0.0,...   \n",
              "...                                                 ...   \n",
              "3253  [0.0, 0.000623866, 0.0008337597, 0.7667371, 0....   \n",
              "3254  [0.0, 0.000493643, 0.019217728, 0.00043680664,...   \n",
              "3255  [0.0, 0.0006225555, 0.0006269635, 0.021234082,...   \n",
              "3256  [0.0, 0.051019795, 0.04164902, 0.039356954, 0....   \n",
              "3257  [0.0, 0.16154839, 0.3690353, 0.34384218, 0.0, ...   \n",
              "\n",
              "                                        predicted_label  \n",
              "0     [3, 0, 3, 4, 4, 4, 0, 0, 0, 0, 3, 3, 3, 3, 3, ...  \n",
              "1     [0, 5, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
              "2     [0, 0, 0, 0, 0, 0, 1, 2, 2, 0, 0, 0, 0, 0, 0, ...  \n",
              "3     [6, 5, 6, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, ...  \n",
              "4     [0, 5, 6, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, ...  \n",
              "...                                                 ...  \n",
              "3253  [0, 0, 0, 3, 3, 4, 0, 0, 0, 5, 6, 6, 0, 0, 0, ...  \n",
              "3254  [0, 0, 5, 0, 0, 0, 0, 0, 1, 2, 2, 0, 1, 2, 2, ...  \n",
              "3255  [0, 0, 0, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
              "3256  [6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...  \n",
              "3257  [4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...  \n",
              "\n",
              "[3258 rows x 5 columns]>"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross Lingual Transfer (Zero Shot Lerning)"
      ],
      "metadata": {
        "id": "zmMy2tRzVmSp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "282hF8KeMZ1A",
        "outputId": "0d05a887-18a2-47cd-feda-317eb4576a9b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 136/136 [02:31<00:00,  1.08it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "eval prediction [[[ 1.46049130e+00 -3.71023789e-02 -5.66521764e-01 ... -6.71732128e-02\n",
            "    9.17637199e-02 -7.39446223e-01]\n",
            "  [ 3.83629084e+00  3.14959109e-01 -3.00060081e+00 ... -2.05165267e+00\n",
            "    1.74679971e+00 -3.00509357e+00]\n",
            "  [ 4.89869308e+00 -3.02554798e+00 -6.37388110e-01 ...  1.96551132e+00\n",
            "   -2.31688643e+00 -1.62800461e-01]\n",
            "  ...\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]]\n",
            "\n",
            " [[ 1.82026315e+00 -3.67643476e-01 -2.34368905e-01 ...  1.25174522e-01\n",
            "   -4.47949737e-01 -7.34403849e-01]\n",
            "  [ 2.94063449e-01  6.40595722e+00 -1.79854214e+00 ... -2.13840580e+00\n",
            "   -7.60737360e-01 -4.04679489e+00]\n",
            "  [-1.01316355e-01 -9.64817047e-01  6.11404943e+00 ...  2.02791357e+00\n",
            "   -2.63783097e+00 -1.15156198e+00]\n",
            "  ...\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]]\n",
            "\n",
            " [[ 2.23778057e+00 -3.14572543e-01 -4.15403843e-01 ... -6.13554567e-02\n",
            "   -4.02813435e-01 -6.76942348e-01]\n",
            "  [ 8.09427261e+00 -1.51089776e+00 -1.53690505e+00 ... -1.54056692e+00\n",
            "   -9.80376840e-01 -1.93256569e+00]\n",
            "  [ 7.76963949e+00 -1.77143025e+00 -1.66512692e+00 ... -1.27286983e+00\n",
            "   -9.96338427e-01 -1.75742507e+00]\n",
            "  ...\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-3.57476860e-01 -9.70112562e-01 -4.52579916e-01 ...  1.88841093e+00\n",
            "   -1.28324300e-01 -2.43390948e-02]\n",
            "  [-1.25686288e+00  2.12611198e+00 -2.92627954e+00 ... -1.43847847e+00\n",
            "    2.74781489e+00 -3.58903027e+00]\n",
            "  [-1.93081546e+00 -2.41510415e+00  1.51375961e+00 ...  4.92146301e+00\n",
            "   -1.98546267e+00  2.58104992e+00]\n",
            "  ...\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]]\n",
            "\n",
            " [[ 3.43502283e-01 -1.73096883e+00 -1.78564000e+00 ...  4.16196346e-01\n",
            "    1.05240369e+00  2.04687738e+00]\n",
            "  [-1.54954016e+00 -6.80414379e-01 -4.21477079e+00 ... -1.12671113e+00\n",
            "    5.91027355e+00 -8.81646037e-01]\n",
            "  [-3.70983183e-01 -3.92117167e+00 -2.99652129e-01 ...  3.35740566e+00\n",
            "   -1.06859004e+00  5.68417883e+00]\n",
            "  ...\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]]\n",
            "\n",
            " [[ 3.39677954e+00  2.40862146e-02  1.47481740e-01 ... -4.41187620e-01\n",
            "   -6.48525715e-01 -1.39307666e+00]\n",
            "  [-1.05942047e+00  6.59737778e+00 -8.44515204e-01 ... -1.67699957e+00\n",
            "   -1.19814467e+00 -3.76063299e+00]\n",
            "  [-1.56230330e+00 -1.02022326e+00  6.58587265e+00 ...  2.10254121e+00\n",
            "   -2.18271685e+00 -1.01213408e+00]\n",
            "  ...\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 136/136 [02:32<00:00,  1.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 Score of [en] model on [en] dataset : 0.779\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#function to get f1 score of some dataset on some trainer\n",
        "def get_f1(trainer,dataset):\n",
        "    return trainer.predict(dataset).metrics[\"test_f1\"]\n",
        "\n",
        "f1_scores = defaultdict(dict)\n",
        "f1_scores[\"en\"][\"en\"] = get_f1(trainer, panx_en_encoded[\"test\"])\n",
        "print(f\"F1 Score of [en] model on [en] dataset : {f1_scores['en']['en']:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "since we fine tuned the model on english only, the f1 score obtained here is same as the one we obtained earlier. so we can make an assumption that wit will be similar for the other languages, if the model is fine tuned on their dataset."
      ],
      "metadata": {
        "id": "6C5aeqZkV0lO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axGeJtrBMZ1A",
        "outputId": "0be79754-7c11-4a81-fc97-44aa1454e5c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_ids: tensor([[     0,    622,  35473,   4932,   1256,  22667,  47582, 105173,     23,\n",
            "          37061,      2]])\n",
            "Shape of Outputs: torch.Size([1, 11, 7])\n",
            "Predictions: tensor([[0, 0, 3, 4, 0, 0, 0, 0, 0, 5, 0]])\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Tokens</th>\n",
              "      <td>&lt;s&gt;</td>\n",
              "      <td>▁Die</td>\n",
              "      <td>▁Deutsche</td>\n",
              "      <td>▁Bank</td>\n",
              "      <td>▁hat</td>\n",
              "      <td>▁ihren</td>\n",
              "      <td>▁Haupt</td>\n",
              "      <td>sitz</td>\n",
              "      <td>▁in</td>\n",
              "      <td>▁Frankfurt</td>\n",
              "      <td>&lt;/s&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NER Tags</th>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>B-ORG</td>\n",
              "      <td>I-ORG</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>B-LOC</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           0     1          2      3     4       5       6     7    8   \\\n",
              "Tokens    <s>  ▁Die  ▁Deutsche  ▁Bank  ▁hat  ▁ihren  ▁Haupt  sitz  ▁in   \n",
              "NER Tags    O     O      B-ORG  I-ORG     O       O       O     O    O   \n",
              "\n",
              "                  9     10  \n",
              "Tokens    ▁Frankfurt  </s>  \n",
              "NER Tags       B-LOC     O  "
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_de = \"Die Deutsche Bank hat ihren Hauptsitz in Frankfurt\"\n",
        "tag_text(text_de, tags, trainer.model, xlmr_tokenizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVxqBaqSMZ1A"
      },
      "outputs": [],
      "source": [
        "def evaluate_lang_performance(lang, trainer):\n",
        "    # Encode the PAN-X dataset for the specified language\n",
        "    panx_ds = encode_panx_dataset(panx_ds_combined[lang])\n",
        "\n",
        "    # Evaluate the trained model using the test set of the encoded PAN-X dataset\n",
        "    return get_f1(trainer, panx_ds[\"test\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Nn5S9BZMZ1B",
        "outputId": "07345a57-63f3-4aa6-c454-dee6ca7ed255"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 4260/4260 [00:00<00:00, 9801.62 examples/s]\n",
            "Map: 100%|██████████| 2130/2130 [00:00<00:00, 12083.20 examples/s]\n",
            "Map: 100%|██████████| 2130/2130 [00:00<00:00, 12186.12 examples/s]\n",
            "100%|██████████| 89/89 [01:46<00:00,  1.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "eval prediction [[[ 1.7654796e+00 -5.3948122e-01 -4.3298650e-01 ... -1.2113470e-01\n",
            "   -7.7366360e-02 -1.5570924e-01]\n",
            "  [ 7.9152718e+00 -1.4468144e+00 -1.5894629e+00 ... -1.1036086e+00\n",
            "   -1.3259628e+00 -2.1514266e+00]\n",
            "  [ 5.9021029e+00 -7.6581806e-01 -1.5934107e+00 ... -1.8940029e+00\n",
            "   -2.1271126e-01 -3.0267739e+00]\n",
            "  ...\n",
            "  [-1.0000000e+02 -1.0000000e+02 -1.0000000e+02 ... -1.0000000e+02\n",
            "   -1.0000000e+02 -1.0000000e+02]\n",
            "  [-1.0000000e+02 -1.0000000e+02 -1.0000000e+02 ... -1.0000000e+02\n",
            "   -1.0000000e+02 -1.0000000e+02]\n",
            "  [-1.0000000e+02 -1.0000000e+02 -1.0000000e+02 ... -1.0000000e+02\n",
            "   -1.0000000e+02 -1.0000000e+02]]\n",
            "\n",
            " [[ 1.6829864e+00 -1.2592244e-01  2.0880729e-02 ... -1.2303443e-01\n",
            "   -4.6750200e-01 -5.4393089e-01]\n",
            "  [ 7.3661137e+00 -1.6074311e+00 -1.6703248e+00 ... -1.1336257e+00\n",
            "   -7.5777811e-01 -2.0468729e+00]\n",
            "  [ 6.3137741e+00 -1.7128105e+00 -1.6979858e+00 ... -9.3677235e-01\n",
            "   -6.0666120e-01 -1.6773344e+00]\n",
            "  ...\n",
            "  [-1.0000000e+02 -1.0000000e+02 -1.0000000e+02 ... -1.0000000e+02\n",
            "   -1.0000000e+02 -1.0000000e+02]\n",
            "  [-1.0000000e+02 -1.0000000e+02 -1.0000000e+02 ... -1.0000000e+02\n",
            "   -1.0000000e+02 -1.0000000e+02]\n",
            "  [-1.0000000e+02 -1.0000000e+02 -1.0000000e+02 ... -1.0000000e+02\n",
            "   -1.0000000e+02 -1.0000000e+02]]\n",
            "\n",
            " [[ 4.6671239e-01 -1.2031894e+00 -5.3043187e-01 ...  2.5845926e+00\n",
            "   -3.5934800e-01 -4.4745827e-01]\n",
            "  [ 2.3708456e+00  1.3164815e+00 -2.7036035e+00 ... -1.1742823e+00\n",
            "    1.4622190e+00 -3.2579894e+00]\n",
            "  [ 9.7697496e-01 -2.0051837e+00  1.2893638e-01 ...  3.2441115e+00\n",
            "   -1.1063355e+00  4.9444377e-01]\n",
            "  ...\n",
            "  [-1.0000000e+02 -1.0000000e+02 -1.0000000e+02 ... -1.0000000e+02\n",
            "   -1.0000000e+02 -1.0000000e+02]\n",
            "  [-1.0000000e+02 -1.0000000e+02 -1.0000000e+02 ... -1.0000000e+02\n",
            "   -1.0000000e+02 -1.0000000e+02]\n",
            "  [-1.0000000e+02 -1.0000000e+02 -1.0000000e+02 ... -1.0000000e+02\n",
            "   -1.0000000e+02 -1.0000000e+02]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 1.3374329e+00 -5.9242930e-02  9.8476246e-02 ... -1.4186060e-01\n",
            "   -3.2056135e-01 -4.0652001e-01]\n",
            "  [ 6.6845603e+00 -1.5239859e+00 -1.9881372e+00 ... -1.6617913e+00\n",
            "    2.4906027e-01 -2.0533566e+00]\n",
            "  [ 7.2875767e+00 -1.6707084e+00 -1.6441431e+00 ... -1.2584953e+00\n",
            "   -9.3951583e-01 -1.6467507e+00]\n",
            "  ...\n",
            "  [-1.0000000e+02 -1.0000000e+02 -1.0000000e+02 ... -1.0000000e+02\n",
            "   -1.0000000e+02 -1.0000000e+02]\n",
            "  [-1.0000000e+02 -1.0000000e+02 -1.0000000e+02 ... -1.0000000e+02\n",
            "   -1.0000000e+02 -1.0000000e+02]\n",
            "  [-1.0000000e+02 -1.0000000e+02 -1.0000000e+02 ... -1.0000000e+02\n",
            "   -1.0000000e+02 -1.0000000e+02]]\n",
            "\n",
            " [[-1.9632387e-01  1.4145988e+00  1.9900610e+00 ...  4.1133919e-01\n",
            "   -7.1160686e-01 -8.5883546e-01]\n",
            "  [-1.4674231e+00  6.7302003e+00 -1.1719950e+00 ... -1.6489050e+00\n",
            "   -1.1545346e+00 -3.5560777e+00]\n",
            "  [-1.4926614e+00 -6.9993764e-02  6.4393883e+00 ...  2.1755250e+00\n",
            "   -2.9668274e+00 -1.1817265e+00]\n",
            "  ...\n",
            "  [-1.0000000e+02 -1.0000000e+02 -1.0000000e+02 ... -1.0000000e+02\n",
            "   -1.0000000e+02 -1.0000000e+02]\n",
            "  [-1.0000000e+02 -1.0000000e+02 -1.0000000e+02 ... -1.0000000e+02\n",
            "   -1.0000000e+02 -1.0000000e+02]\n",
            "  [-1.0000000e+02 -1.0000000e+02 -1.0000000e+02 ... -1.0000000e+02\n",
            "   -1.0000000e+02 -1.0000000e+02]]\n",
            "\n",
            " [[ 1.6261263e+00  1.1335188e-01  2.8013760e-01 ... -1.8638512e-01\n",
            "   -6.4215767e-01 -6.9626379e-01]\n",
            "  [ 7.7757368e+00 -1.4517807e+00 -1.6612029e+00 ... -1.7147416e+00\n",
            "   -1.1033418e+00 -2.0368221e+00]\n",
            "  [ 7.8254395e+00 -2.2180033e+00 -1.1553385e+00 ... -1.1130809e+00\n",
            "   -1.5662959e+00 -1.2909739e+00]\n",
            "  ...\n",
            "  [-1.0000000e+02 -1.0000000e+02 -1.0000000e+02 ... -1.0000000e+02\n",
            "   -1.0000000e+02 -1.0000000e+02]\n",
            "  [-1.0000000e+02 -1.0000000e+02 -1.0000000e+02 ... -1.0000000e+02\n",
            "   -1.0000000e+02 -1.0000000e+02]\n",
            "  [-1.0000000e+02 -1.0000000e+02 -1.0000000e+02 ... -1.0000000e+02\n",
            "   -1.0000000e+02 -1.0000000e+02]]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 89/89 [01:46<00:00,  1.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 Score of [en] model on [de] dataset : 0.721\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "f1_scores[\"en\"][\"de\"] = evaluate_lang_performance(\"de\", trainer)\n",
        "print(f\"F1 Score of [en] model on [de] dataset : {f1_scores['en']['de']:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9waA543HMZ1B",
        "outputId": "c4be533a-934f-4c36-da68-a8aeddba5e48"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 5263/5263 [00:00<00:00, 15126.66 examples/s]\n",
            "Map: 100%|██████████| 2631/2631 [00:00<00:00, 13920.36 examples/s]\n",
            "Map: 100%|██████████| 2631/2631 [00:00<00:00, 15315.82 examples/s]\n",
            "100%|██████████| 110/110 [01:52<00:00,  1.16it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "eval prediction [[[ 1.1729939e+00  2.7410325e-01  8.8141453e-01 ... -2.2013556e-02\n",
            "   -3.1532711e-01 -9.0459645e-01]\n",
            "  [-1.3996539e+00  6.6986141e+00 -1.1572733e+00 ... -1.7158077e+00\n",
            "   -7.2285783e-01 -3.7401047e+00]\n",
            "  [-1.2389549e+00 -8.7883365e-01  6.6923647e+00 ...  1.6943277e+00\n",
            "   -2.3795376e+00 -1.0277795e+00]\n",
            "  ...\n",
            "  [-1.0000000e+02 -1.0000000e+02 -1.0000000e+02 ... -1.0000000e+02\n",
            "   -1.0000000e+02 -1.0000000e+02]\n",
            "  [-1.0000000e+02 -1.0000000e+02 -1.0000000e+02 ... -1.0000000e+02\n",
            "   -1.0000000e+02 -1.0000000e+02]\n",
            "  [-1.0000000e+02 -1.0000000e+02 -1.0000000e+02 ... -1.0000000e+02\n",
            "   -1.0000000e+02 -1.0000000e+02]]\n",
            "\n",
            " [[ 2.9885125e-01 -6.8031669e-01 -1.3299687e-01 ...  1.2182331e+00\n",
            "   -5.3166848e-01 -3.1925291e-01]\n",
            "  [-6.8636402e-02  2.4938121e+00 -3.2208400e+00 ... -1.4235836e+00\n",
            "    1.6480844e+00 -3.6147037e+00]\n",
            "  [-1.2832096e+00 -2.1283283e+00  1.3356602e+00 ...  4.6180525e+00\n",
            "   -2.3879809e+00  1.0397177e+00]\n",
            "  ...\n",
            "  [-1.0000000e+02 -1.0000000e+02 -1.0000000e+02 ... -1.0000000e+02\n",
            "   -1.0000000e+02 -1.0000000e+02]\n",
            "  [-1.0000000e+02 -1.0000000e+02 -1.0000000e+02 ... -1.0000000e+02\n",
            "   -1.0000000e+02 -1.0000000e+02]\n",
            "  [-1.0000000e+02 -1.0000000e+02 -1.0000000e+02 ... -1.0000000e+02\n",
            "   -1.0000000e+02 -1.0000000e+02]]\n",
            "\n",
            " [[ 4.5138833e-01 -4.0313566e-01 -1.8157527e-02 ...  4.9371219e-01\n",
            "   -3.2403681e-01  4.3608554e-02]\n",
            "  [-6.3641870e-01  1.9460562e+00 -2.5794137e+00 ... -1.3736455e+00\n",
            "    2.0762420e+00 -3.4063220e+00]\n",
            "  [-1.3432397e+00 -1.8796661e+00  1.6599698e+00 ...  3.6551178e+00\n",
            "   -1.4416994e+00  1.3197488e+00]\n",
            "  ...\n",
            "  [-1.0000000e+02 -1.0000000e+02 -1.0000000e+02 ... -1.0000000e+02\n",
            "   -1.0000000e+02 -1.0000000e+02]\n",
            "  [-1.0000000e+02 -1.0000000e+02 -1.0000000e+02 ... -1.0000000e+02\n",
            "   -1.0000000e+02 -1.0000000e+02]\n",
            "  [-1.0000000e+02 -1.0000000e+02 -1.0000000e+02 ... -1.0000000e+02\n",
            "   -1.0000000e+02 -1.0000000e+02]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 1.3309985e-01 -1.1704242e+00 -2.6480243e-01 ...  2.3665013e+00\n",
            "   -9.0120524e-01  1.6130634e-01]\n",
            "  [ 2.7639684e-01  1.5543654e+00 -2.4725573e+00 ... -1.0594604e+00\n",
            "    1.8027724e+00 -2.9051886e+00]\n",
            "  [-8.6493254e-01 -2.2171085e+00  8.6147797e-01 ...  4.3604345e+00\n",
            "   -1.9385798e+00  1.1817447e+00]\n",
            "  ...\n",
            "  [-1.0000000e+02 -1.0000000e+02 -1.0000000e+02 ... -1.0000000e+02\n",
            "   -1.0000000e+02 -1.0000000e+02]\n",
            "  [-1.0000000e+02 -1.0000000e+02 -1.0000000e+02 ... -1.0000000e+02\n",
            "   -1.0000000e+02 -1.0000000e+02]\n",
            "  [-1.0000000e+02 -1.0000000e+02 -1.0000000e+02 ... -1.0000000e+02\n",
            "   -1.0000000e+02 -1.0000000e+02]]\n",
            "\n",
            " [[ 1.7505183e+00 -1.9951849e-01 -7.0194721e-01 ...  1.0061176e+00\n",
            "   -1.7657568e-01 -1.3704422e+00]\n",
            "  [ 7.7293205e+00 -1.7479999e+00 -1.8938566e+00 ... -1.1226199e+00\n",
            "   -1.3682948e+00 -1.8212841e+00]\n",
            "  [ 7.6182432e+00 -2.3857961e+00 -1.7263906e+00 ... -3.5517853e-01\n",
            "   -1.5847059e+00 -1.6158000e+00]\n",
            "  ...\n",
            "  [-1.0000000e+02 -1.0000000e+02 -1.0000000e+02 ... -1.0000000e+02\n",
            "   -1.0000000e+02 -1.0000000e+02]\n",
            "  [-1.0000000e+02 -1.0000000e+02 -1.0000000e+02 ... -1.0000000e+02\n",
            "   -1.0000000e+02 -1.0000000e+02]\n",
            "  [-1.0000000e+02 -1.0000000e+02 -1.0000000e+02 ... -1.0000000e+02\n",
            "   -1.0000000e+02 -1.0000000e+02]]\n",
            "\n",
            " [[ 9.3640816e-01  1.2226556e-01  7.5486851e-01 ...  1.2211584e-01\n",
            "   -1.7345852e-01 -3.6075097e-01]\n",
            "  [-8.3177221e-01  6.1634474e+00 -1.8131014e+00 ... -2.2440901e+00\n",
            "    7.5011075e-01 -3.6151502e+00]\n",
            "  [-1.6278689e+00  3.6661753e-01  4.4365659e+00 ...  1.4188246e+00\n",
            "   -1.7157874e+00  5.3494191e-01]\n",
            "  ...\n",
            "  [-1.0000000e+02 -1.0000000e+02 -1.0000000e+02 ... -1.0000000e+02\n",
            "   -1.0000000e+02 -1.0000000e+02]\n",
            "  [-1.0000000e+02 -1.0000000e+02 -1.0000000e+02 ... -1.0000000e+02\n",
            "   -1.0000000e+02 -1.0000000e+02]\n",
            "  [-1.0000000e+02 -1.0000000e+02 -1.0000000e+02 ... -1.0000000e+02\n",
            "   -1.0000000e+02 -1.0000000e+02]]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 110/110 [01:52<00:00,  1.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 Score of [en] model on [fr] dataset : 0.749\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "f1_scores[\"en\"][\"fr\"] = evaluate_lang_performance(\"fr\", trainer)\n",
        "print(f\"F1 Score of [en] model on [fr] dataset : {f1_scores['en']['fr']:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1kfkz_VMZ1B",
        "outputId": "184e29c4-332d-455d-af97-a0686dedfbad"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 2055/2055 [00:00<00:00, 11241.92 examples/s]\n",
            "Map: 100%|██████████| 1027/1027 [00:00<00:00, 11424.95 examples/s]\n",
            "Map: 100%|██████████| 1027/1027 [00:00<00:00, 11729.72 examples/s]\n",
            "100%|██████████| 43/43 [00:43<00:00,  1.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "eval prediction [[[ 1.24791801e+00 -6.44773617e-03  2.00187206e-01 ... -5.00802398e-02\n",
            "   -4.15002704e-01 -4.44445252e-01]\n",
            "  [ 8.10542774e+00 -1.87346852e+00 -1.29669189e+00 ... -1.00024939e+00\n",
            "   -1.37954640e+00 -1.57459366e+00]\n",
            "  [-1.33458996e+00  6.41132069e+00 -8.97890806e-01 ... -1.62227225e+00\n",
            "   -9.66181517e-01 -3.89935017e+00]\n",
            "  ...\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]]\n",
            "\n",
            " [[-3.29244375e-01 -3.91216576e-01  4.52060759e-01 ...  8.12653601e-01\n",
            "   -1.13679074e-01  5.49406528e-01]\n",
            "  [-1.51808143e+00  3.04120874e+00 -2.17342997e+00 ... -1.80968094e+00\n",
            "    3.10376549e+00 -3.18472695e+00]\n",
            "  [-1.95961642e+00 -1.75708246e+00  3.05272388e+00 ...  3.36874843e+00\n",
            "   -1.77071524e+00  2.33589578e+00]\n",
            "  ...\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]]\n",
            "\n",
            " [[ 3.26370001e+00 -1.21633708e-03 -7.12495029e-01 ... -3.20341349e-01\n",
            "   -1.76371664e-01 -1.23829973e+00]\n",
            "  [-7.98578382e-01  5.12073803e+00 -2.73942947e+00 ... -2.46301174e+00\n",
            "    1.77939749e+00 -3.87223053e+00]\n",
            "  [-1.68841863e+00 -4.03483212e-03  3.55123520e+00 ...  2.35967064e+00\n",
            "   -1.79148531e+00  1.18629861e+00]\n",
            "  ...\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 1.01249242e+00  6.42164946e-02 -5.20236850e-01 ... -7.20278621e-02\n",
            "    3.92121255e-01 -4.66875643e-01]\n",
            "  [ 7.70991135e+00 -1.96248233e+00 -1.79535627e+00 ... -1.13866413e+00\n",
            "   -1.35526037e+00 -1.68198311e+00]\n",
            "  [ 7.76265335e+00 -2.30205965e+00 -1.64660323e+00 ... -9.64430571e-01\n",
            "   -1.48842907e+00 -1.60256660e+00]\n",
            "  ...\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]]\n",
            "\n",
            " [[ 1.59050679e+00  8.18244398e-01  8.61651003e-02 ... -1.08910963e-01\n",
            "   -3.82875651e-01 -1.14262784e+00]\n",
            "  [ 7.86424828e+00 -1.95767176e+00 -1.71034777e+00 ... -1.10534716e+00\n",
            "   -1.37714374e+00 -1.66954660e+00]\n",
            "  [ 7.91057301e+00 -2.18949223e+00 -1.73587728e+00 ... -1.06819105e+00\n",
            "   -1.43512917e+00 -1.74693310e+00]\n",
            "  ...\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]]\n",
            "\n",
            " [[ 2.07574606e+00  4.59093750e-01 -3.75019431e-01 ... -2.76335776e-01\n",
            "   -1.84680209e-01 -1.30120587e+00]\n",
            "  [-9.60291445e-01  6.51399422e+00 -1.86056876e+00 ... -1.62751317e+00\n",
            "   -3.91942799e-01 -3.92136526e+00]\n",
            "  [-2.19526720e+00  2.84213495e+00  3.78694940e+00 ...  1.82840085e+00\n",
            "   -2.99178123e+00 -1.52091479e+00]\n",
            "  ...\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]]]\n",
            "F1 Score of [en] model on [it] dataset : 0.766\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "f1_scores[\"en\"][\"it\"] = evaluate_lang_performance(\"it\", trainer)\n",
        "print(f\"F1 Score of [en] model on [it] dataset : {f1_scores['en']['it']:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwM-N496MZ1B",
        "outputId": "9ac5d96a-edc1-4c84-9f52-7fb03dba7713"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 1904/1904 [00:00<00:00, 11255.92 examples/s]\n",
            "Map: 100%|██████████| 952/952 [00:00<00:00, 13727.80 examples/s]\n",
            "Map: 100%|██████████| 952/952 [00:00<00:00, 12933.43 examples/s]\n",
            "100%|██████████| 40/40 [00:29<00:00,  1.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "eval prediction [[[ 7.02387333e-01  4.58234370e-01  2.37537190e-01 ...  5.70428610e-01\n",
            "   -3.42528522e-01 -1.39344597e+00]\n",
            "  [ 7.12630987e+00 -1.49028051e+00 -9.70057011e-01 ... -7.07740188e-01\n",
            "   -1.28223944e+00 -2.15962958e+00]\n",
            "  [-1.26061380e+00  6.23447180e+00 -1.33859730e+00 ... -1.30007482e+00\n",
            "   -7.03920484e-01 -3.83252859e+00]\n",
            "  ...\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]]\n",
            "\n",
            " [[ 5.14903069e-01 -9.47425440e-02 -2.18583584e-01 ...  1.31150514e-01\n",
            "    3.12980294e-01 -1.10850945e-01]\n",
            "  [ 7.11149836e+00 -1.85019588e+00 -1.41997623e+00 ... -7.93622613e-01\n",
            "   -9.06164944e-01 -1.65209866e+00]\n",
            "  [-1.25735533e+00  3.01985312e+00 -2.80025005e+00 ... -2.25481963e+00\n",
            "    4.52096796e+00 -2.73410296e+00]\n",
            "  ...\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]]\n",
            "\n",
            " [[ 1.95983553e+00 -7.47701168e-01 -6.73053384e-01 ... -1.94837898e-02\n",
            "    1.98866308e-01 -1.52903527e-01]\n",
            "  [ 9.43846703e-01  7.14872658e-01 -3.35334778e+00 ... -1.96106684e+00\n",
            "    4.50766563e+00 -3.00233507e+00]\n",
            "  [ 1.18955404e-01 -3.91954470e+00 -9.42832291e-01 ...  2.53565311e+00\n",
            "    3.36137593e-01  4.32189655e+00]\n",
            "  ...\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 1.71724975e+00 -1.82019114e-01 -9.16118741e-01 ...  5.64653397e-01\n",
            "   -3.21390659e-01 -1.34829772e+00]\n",
            "  [ 7.96666527e+00 -2.03358412e+00 -1.81493831e+00 ... -1.00916302e+00\n",
            "   -1.37603724e+00 -1.65173662e+00]\n",
            "  [ 7.96417522e+00 -2.15788460e+00 -1.95076466e+00 ... -1.10537815e+00\n",
            "   -1.31828475e+00 -1.82538199e+00]\n",
            "  ...\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]]\n",
            "\n",
            " [[ 1.81419945e+00 -4.35661197e-01 -4.66183960e-01 ... -1.39586449e-01\n",
            "   -1.62012309e-01 -3.10270697e-01]\n",
            "  [ 7.75120831e+00 -2.28658628e+00 -2.35338831e+00 ... -8.17994118e-01\n",
            "   -5.26349425e-01 -1.15677202e+00]\n",
            "  [-1.27897382e+00 -6.53516531e-01 -3.18378711e+00 ... -1.07187808e+00\n",
            "    5.88875437e+00 -1.40855980e+00]\n",
            "  ...\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]]\n",
            "\n",
            " [[ 1.78275716e+00  4.63045835e-01  3.27725530e-01 ...  1.54065415e-02\n",
            "   -3.24133784e-01 -8.69046330e-01]\n",
            "  [ 7.93339586e+00 -1.95245945e+00 -1.77934504e+00 ... -1.00632966e+00\n",
            "   -1.44226336e+00 -1.67951810e+00]\n",
            "  [ 7.89471388e+00 -2.17726135e+00 -1.83503103e+00 ... -9.97539520e-01\n",
            "   -1.43724465e+00 -1.75164318e+00]\n",
            "  ...\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]\n",
            "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 ... -1.00000000e+02\n",
            "   -1.00000000e+02 -1.00000000e+02]]]\n",
            "F1 Score of [en] model on [es] dataset : 0.679\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "f1_scores[\"en\"][\"es\"] = evaluate_lang_performance(\"es\", trainer)\n",
        "print(f\"F1 Score of [en] model on [es] dataset : {f1_scores['en']['es']:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the f1 scores obtained for german, french, italian, spanish are 0.72,0.75,0.77,0.68 respectively, which are quite similar to that of the english dataset. thus we infer that maybe fine tuning the model seperately on these datasets would be a much costlier process for a marginally better result. with better tuning and optimisation, we could get similar or even better results with zero shot learning"
      ],
      "metadata": {
        "id": "jW08OlQ2WQt8"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3ab13f1392f94356b31ae312335ff9a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d44845bdded4bac88dd0a06fe7f5692",
            "placeholder": "​",
            "style": "IPY_MODEL_d633f3e5dea9488e8d24622d75ae0da4",
            "value": "Map: 100%"
          }
        },
        "73c11a9d954d4747a0c9045803226d2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e67c714357bc43c79249f1a2fd019b2f",
            "placeholder": "​",
            "style": "IPY_MODEL_7933b7211a1648bba3a24405ebbdd0ea",
            "value": " 3258/3258 [00:01&lt;00:00, 2076.98 examples/s]"
          }
        },
        "7933b7211a1648bba3a24405ebbdd0ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d44845bdded4bac88dd0a06fe7f5692": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e7afadeddf74d699dd5923e4603e550": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb251395fd324734802de9dfccabdaf0",
            "max": 3258,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e4355bf063b14f4a93c00f23aa1b998d",
            "value": 3258
          }
        },
        "9bcbe32efd1642d88dc9171d4fd7450d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d633f3e5dea9488e8d24622d75ae0da4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e4355bf063b14f4a93c00f23aa1b998d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e67c714357bc43c79249f1a2fd019b2f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb251395fd324734802de9dfccabdaf0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffd2e047db9343f0b7c7d135291f2d27": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3ab13f1392f94356b31ae312335ff9a0",
              "IPY_MODEL_8e7afadeddf74d699dd5923e4603e550",
              "IPY_MODEL_73c11a9d954d4747a0c9045803226d2f"
            ],
            "layout": "IPY_MODEL_9bcbe32efd1642d88dc9171d4fd7450d"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}